{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperSpectral YOLO Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.59  Python-3.12.6 torch-2.6.0.dev20241216+cu124 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)\n",
      "Setup complete  (12 CPUs, 31.9 GB RAM, 772.3/930.9 GB disk)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "import matlab.engine\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "# from google.colab import files\n",
    "# from google.colab import drive\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from ultralytics import YOLO, checks\n",
    "import torch\n",
    "checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = matlab.engine.start_matlab()\n",
    "eng.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_anomalies(image_path, dictionary_path='hs_dictionaries.mat', numb_pca_components =4,num_ica_components = 4, ICA_max_iterations = 1000, similarity_threshold =1, opts_lambda = 0.4, opts_max_iter = 100, lambda_ = 0.01, overlap_threshold = 0.2, min_area = 50,max_area=3200, min_anomalous_pixels = 20, spar =28):\n",
    "    # Start MATLAB engine\n",
    "    \n",
    "\n",
    "    # Convert RGB to hyperspectral (rec_hs is returned directly)\n",
    "    rec_hs = eng.convertToHyperspectral(image_path, dictionary_path, spar)\n",
    "\n",
    "    # Perform anomaly detection\n",
    "    bounding_boxes = eng.anomalyDetection(\n",
    "        rec_hs,\n",
    "        numb_pca_components,\n",
    "        num_ica_components,\n",
    "        ICA_max_iterations,\n",
    "        similarity_threshold,\n",
    "        opts_lambda,\n",
    "        opts_max_iter,\n",
    "        lambda_,\n",
    "        overlap_threshold,\n",
    "        min_area,\n",
    "        max_area,\n",
    "        min_anomalous_pixels\n",
    "        \n",
    "    )\n",
    "    # Convert MATLAB bounding boxes to Python list\n",
    "    bounding_boxes = [list(bbox) for bbox in bounding_boxes]\n",
    "\n",
    "    return bounding_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection using YOLOv8 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correct path to the trained weights\n",
    "\n",
    "# Correct path to the trained weights\n",
    "model = YOLO(r\"yolov8m-drone.pt\")\n",
    "\n",
    "# Path to testing folder\n",
    "test_images_dir = r\"C:\\Users\\michael\\Desktop\\f_proj\\output_images\\test\"  # Folder containing test images\n",
    "results_output_dir = r\"C:\\Users\\michael\\Desktop\\f_proj\\YOLO_test_resultsoriginal\"  # Directory to save results\n",
    "os.makedirs(results_output_dir, exist_ok=True)\n",
    "\n",
    "# Run inference on the testing folder\n",
    "results = model.predict(\n",
    "    source=test_images_dir,  # Path to test images\n",
    "    save=True,               # Save annotated images\n",
    "    save_txt=True,           # Save predictions in text format\n",
    "    project=results_output_dir,  # Save results in this folder\n",
    "    imgsz=640                # Image size\n",
    ")\n",
    "\n",
    "# Print summary of results\n",
    "print(\"Summary of Detection Results:\")\n",
    "print(f\"Number of images tested: {len(results)}\")\n",
    "print(f\"Results saved in: {results_output_dir}\")\n",
    "\n",
    "# Display per-image metrics\n",
    "for result in results:\n",
    "    print(f\"Image: {result.path}, Detections: {len(result.boxes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection using our Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    # boxA and boxB: (x1, y1, x2, y2)\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def is_contained(boxA, boxB):\n",
    "    # Checks if boxA is at least 50% contained in boxB or vice versa.\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    areaA = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    areaB = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    if (interArea / areaA >= 0.5) or (interArea / areaB >= 0.5):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Optional unified confidence threshold:\n",
    "CONF_THRESHOLD = 0.25\n",
    "\n",
    "# Adjustable thresholds:\n",
    "YOLO_OVERLAP_THRESH = 0.0  # for discarding pipeline boxes based on overlap with full-image YOLO boxes\n",
    "PIPELINE_DEDUP_THRESH = 0.5  # for deduplication in pipeline NMS\n",
    "\n",
    "def cross_class_nms(detections, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    A simple cross-class NMS that keeps only the highest-confidence detection\n",
    "    among overlapping boxes (IoU >= iou_thresh).\n",
    "    Each detection is (class_id, confidence, x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    detections = sorted(detections, key=lambda d: d[1], reverse=True)\n",
    "    final_dets = []\n",
    "    used = [False] * len(detections)\n",
    "    for i in range(len(detections)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        final_dets.append(detections[i])\n",
    "        for j in range(i + 1, len(detections)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            iou_val = compute_iou(detections[i][2:6], detections[j][2:6])\n",
    "            if iou_val >= iou_thresh:\n",
    "                used[j] = True\n",
    "    return final_dets\n",
    "\n",
    "def merge_bounding_boxes(bboxes, proximity_threshold=20, max_area=30000):\n",
    "    \"\"\"\n",
    "    Merge bounding boxes (list of (x, y, w, h)) if their centers are within proximity_threshold.\n",
    "    Merge is done only if the new area <= max_area.\n",
    "    Returns merged boxes as (x, y, w, h).\n",
    "    \"\"\"\n",
    "    if not bboxes:\n",
    "        return []\n",
    "    merged = []\n",
    "    used = [False] * len(bboxes)\n",
    "    for i in range(len(bboxes)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        x, y, w, h = bboxes[i]\n",
    "        merged_box = [x, y, x + w, y + h]\n",
    "        used[i] = True\n",
    "        for j in range(i + 1, len(bboxes)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            x2, y2, w2, h2 = bboxes[j]\n",
    "            centerA = ((merged_box[0] + merged_box[2]) / 2, (merged_box[1] + merged_box[3]) / 2)\n",
    "            centerB = (x2 + w2 / 2, y2 + h2 / 2)\n",
    "            dist = ((centerA[0] - centerB[0])**2 + (centerA[1] - centerB[1])**2)**0.5\n",
    "            if dist <= proximity_threshold:\n",
    "                new_x1 = min(merged_box[0], x2)\n",
    "                new_y1 = min(merged_box[1], y2)\n",
    "                new_x2 = max(merged_box[2], x2 + w2)\n",
    "                new_y2 = max(merged_box[3], y2 + h2)\n",
    "                new_w = new_x2 - new_x1\n",
    "                new_h = new_y2 - new_y1\n",
    "                if new_w * new_h <= max_area:\n",
    "                    merged_box = [new_x1, new_y1, new_x2, new_y2]\n",
    "                    used[j] = True\n",
    "        merged.append((merged_box[0], merged_box[1], merged_box[2]-merged_box[0], merged_box[3]-merged_box[1]))\n",
    "    return merged\n",
    "\n",
    "def deduplicate_pipeline_dets(detections, overlap_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Deduplicate detections (list of (class, conf, x1, y1, x2, y2)) by discarding duplicates\n",
    "    if IoU >= overlap_thresh or one box is contained in another.\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "    detections = sorted(detections, key=lambda d: d[1], reverse=True)\n",
    "    final_dets = []\n",
    "    used = [False] * len(detections)\n",
    "    for i in range(len(detections)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        det_i = detections[i]\n",
    "        final_dets.append(det_i)\n",
    "        for j in range(i+1, len(detections)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            if compute_iou(det_i[2:6], detections[j][2:6]) >= overlap_thresh or is_contained(det_i[2:6], detections[j][2:6]):\n",
    "                used[j] = True\n",
    "    return final_dets\n",
    "\n",
    "# *** Vectorized IoU Computation for Re-adding Non-Drone Detections ***\n",
    "def compute_iou_vectorized(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Compute the IoU matrix between two sets of boxes.\n",
    "    boxes1: numpy array of shape (N, 4)\n",
    "    boxes2: numpy array of shape (M, 4)\n",
    "    Returns an array of shape (N, M) with IoU values.\n",
    "    \"\"\"\n",
    "    boxes1 = np.array(boxes1)  # shape (N, 4)\n",
    "    boxes2 = np.array(boxes2)  # shape (M, 4)\n",
    "    \n",
    "    # Compute intersection coordinates\n",
    "    xA = np.maximum(boxes1[:, None, 0], boxes2[None, :, 0])\n",
    "    yA = np.maximum(boxes1[:, None, 1], boxes2[None, :, 1])\n",
    "    xB = np.minimum(boxes1[:, None, 2], boxes2[None, :, 2])\n",
    "    yB = np.minimum(boxes1[:, None, 3], boxes2[None, :, 3])\n",
    "    \n",
    "    interWidth = np.maximum(0, xB - xA + 1)\n",
    "    interHeight = np.maximum(0, yB - yA + 1)\n",
    "    interArea = interWidth * interHeight\n",
    "    \n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0] + 1) * (boxes1[:, 3] - boxes1[:, 1] + 1)\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n",
    "    \n",
    "    unionArea = area1[:, None] + area2[None, :] - interArea\n",
    "    iou_matrix = interArea / unionArea\n",
    "    return iou_matrix\n",
    "\n",
    "# Main processing\n",
    "paddings = [200]\n",
    "model = YOLO(r\"yolov8m-drone.pt\")\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "for padding in paddings:\n",
    "    test_images_dir = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\train\\images\"  # Folder containing test images\n",
    "    results_output_dir = fr\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\trainresults\"\n",
    "    os.makedirs(results_output_dir, exist_ok=True)\n",
    "    labels_output_dir = os.path.join(results_output_dir, \"labels\")\n",
    "    os.makedirs(labels_output_dir, exist_ok=True)\n",
    "    \n",
    "    test_images = [os.path.join(test_images_dir, img) for img in os.listdir(test_images_dir)]\n",
    "    for image_path in test_images:\n",
    "        print(f\"Processing: {image_path}\")\n",
    "        results = model.predict(source=image_path, imgsz=640, conf=CONF_THRESHOLD, save=False)\n",
    "        yolo_labels = []\n",
    "        detections = results[0].boxes  # YOLO detections on the full image\n",
    "        original_image = cv2.imread(image_path)\n",
    "        \n",
    "        # *** NEW: Keep non-drone YOLO detections separately\n",
    "        yolo_detections = []\n",
    "        non_drone_dets = []\n",
    "        if detections is not None and len(detections) > 0:\n",
    "            for box in detections:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                class_id = int(box.cls[0])\n",
    "                confidence = float(box.conf[0])\n",
    "                print(f\"Full-image YOLO: Class {class_id}, Conf {confidence:.2f}, BBox ({x1}, {y1}, {x2}, {y2})\")\n",
    "                if class_id == 0:\n",
    "                    yolo_detections.append((x1, y1, x2, y2, class_id, confidence))\n",
    "                    # Save with \"y\" prefix\n",
    "                    x_center = (x1 + x2) / 2 / original_image.shape[1]\n",
    "                    y_center = (y1 + y2) / 2 / original_image.shape[0]\n",
    "                    bbox_width = (x2 - x1) / original_image.shape[1]\n",
    "                    bbox_height = (y2 - y1) / original_image.shape[0]\n",
    "                    yolo_labels.append(f\"y {class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\")\n",
    "                else:\n",
    "                    non_drone_dets.append((x1, y1, x2, y2, class_id, confidence))\n",
    "            print(f\"Drones (class=0) detected in {image_path} by YOLO: {len(yolo_detections)}\")\n",
    "        else:\n",
    "            print(f\"No YOLO detections in {image_path}.\")\n",
    "        \n",
    "        # Draw YOLO drone boxes\n",
    "        for (x1, y1, x2, y2, cls_id, conf) in yolo_detections:\n",
    "            cv2.rectangle(original_image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "        \n",
    "        # 2) Run anomaly detection pipeline\n",
    "        try:\n",
    "            bounding_boxes = detect_anomalies(image_path, max_area=30000, numb_pca_components=4)\n",
    "            merged_boxes = merge_bounding_boxes(bounding_boxes, proximity_threshold=20, max_area=30000)\n",
    "            merged_boxes = merge_bounding_boxes(merged_boxes, proximity_threshold=20, max_area=30000)\n",
    "            \n",
    "            pipeline_detections_global = []\n",
    "            # For each merged anomaly bounding box, run YOLO on the ROI\n",
    "            for bbox in merged_boxes:\n",
    "                x, y, w, h = map(int, bbox)\n",
    "                # Compute padded ROI (before zooming)\n",
    "                x_padded = max(x - padding, 0)\n",
    "                y_padded = max(y - padding, 0)\n",
    "                w_padded = min(w + 2 * padding, original_image.shape[1] - x_padded)\n",
    "                h_padded = min(h + 2 * padding, original_image.shape[0] - y_padded)\n",
    "                \n",
    "                # -------------------------------\n",
    "                # ROI CENTERING ADJUSTMENT BLOCK (1.1)\n",
    "                orig_center_x = x + w / 2\n",
    "                orig_center_y = y + h / 2\n",
    "                padded_center_x = x_padded + w_padded / 2\n",
    "                padded_center_y = y_padded + h_padded / 2\n",
    "                offset_x = orig_center_x - padded_center_x\n",
    "                offset_y = orig_center_y - padded_center_y\n",
    "                x_padded = max(int(x_padded + offset_x), 0)\n",
    "                y_padded = max(int(y_padded + offset_y), 0)\n",
    "                # Recalculate ROI width and height to remain within image bounds\n",
    "                w_padded = min(w + 2 * padding, original_image.shape[1] - x_padded)\n",
    "                h_padded = min(h + 2 * padding, original_image.shape[0] - y_padded)\n",
    "                # -------------------------------\n",
    "                print(f\"ROI before zooming: (x={x_padded}, y={y_padded}, w={w_padded}, h={h_padded})\")\n",
    "                \n",
    "                # -------------------------------\n",
    "                # ZOOMING BLOCK\n",
    "                zoom_factor = 3 if (w_padded < 200 or h_padded < 200) else 2\n",
    "                if zoom_factor > 2:\n",
    "                    roi = original_image[y_padded:y_padded+h_padded, x_padded:x_padded+w_padded]\n",
    "                    roi_zoomed = cv2.resize(roi, (roi.shape[1]*zoom_factor, roi.shape[0]*zoom_factor))\n",
    "                    roi_for_yolo = cv2.resize(roi_zoomed, (640, 640))\n",
    "                    effective_w = w_padded * zoom_factor\n",
    "                    effective_h = h_padded * zoom_factor\n",
    "                    print(f\"Zooming applied with factor {zoom_factor}.\")\n",
    "                else:\n",
    "                    roi = original_image[y_padded:y_padded+h_padded, x_padded:x_padded+w_padded]\n",
    "                    roi_for_yolo = cv2.resize(roi, (640, 640))\n",
    "                    effective_w = w_padded\n",
    "                    effective_h = h_padded\n",
    "                # -------------------------------\n",
    "                \n",
    "                roi_results = model.predict(source=roi_for_yolo, conf=CONF_THRESHOLD, save=False)\n",
    "                roi_detections = roi_results[0].boxes\n",
    "                if roi_detections is not None and len(roi_detections) > 0:\n",
    "                    print(f\"Anomaly ROI from {image_path}: (x={x_padded}, y={y_padded}, w={w_padded}, h={h_padded})\")\n",
    "                    roi_dets_list = []\n",
    "                    for roi_box in roi_detections:\n",
    "                        x1_r, y1_r, x2_r, y2_r = map(int, roi_box.xyxy[0])\n",
    "                        conf_r = float(roi_box.conf[0])\n",
    "                        cls_r = int(roi_box.cls[0])\n",
    "                        print(f\"  ROI detection: Class {cls_r}, Conf {conf_r:.2f}, BBox ({x1_r}, {y1_r}, {x2_r}, {y2_r})\")\n",
    "                        roi_dets_list.append((cls_r, conf_r, x1_r, y1_r, x2_r, y2_r))\n",
    "                    filtered_dets = cross_class_nms(roi_dets_list, iou_thresh=0.25)\n",
    "                    for (cls_r, conf_r, x1_r, y1_r, x2_r, y2_r) in filtered_dets:\n",
    "                        if cls_r == 0:\n",
    "                            x1_abs = x1_r * effective_w // 640 + x_padded\n",
    "                            x2_abs = x2_r * effective_w // 640 + x_padded\n",
    "                            y1_abs = y1_r * effective_h // 640 + y_padded\n",
    "                            y2_abs = y2_r * effective_h // 640 + y_padded\n",
    "                            pipeline_box = (x1_abs, y1_abs, x2_abs, y2_abs)\n",
    "                            discard_pipeline = False\n",
    "                            for yd in yolo_detections:\n",
    "                                if compute_iou(pipeline_box, yd[:4]) > YOLO_OVERLAP_THRESH or is_contained(pipeline_box, yd[:4]):\n",
    "                                    discard_pipeline = True\n",
    "                                    break\n",
    "                            if not discard_pipeline:\n",
    "                                # NEW: Aspect Ratio Filter\n",
    "                                box_width = x2_abs - x1_abs\n",
    "                                box_height = y2_abs - y1_abs\n",
    "                                ratio = box_width / box_height if box_height > 0 else 0\n",
    "                                if ratio < 0.17 or ratio > 1.52:\n",
    "                                    print(f\"Discarding pipeline detection due to aspect ratio {ratio:.2f}\")\n",
    "                                    continue\n",
    "                                print(f\"  -> Pipeline DRONE candidate: Conf={conf_r:.2f}, Box=({x1_abs}, {y1_abs}, {x2_abs}, {y2_abs})\")\n",
    "                                pipeline_detections_global.append((cls_r, conf_r, x1_abs, y1_abs, x2_abs, y2_abs))\n",
    "                else:\n",
    "                    print(f\"No ROI detections in ROI from {image_path}.\")\n",
    "            \n",
    "            dedup_pipeline_dets = deduplicate_pipeline_dets(pipeline_detections_global, overlap_thresh=PIPELINE_DEDUP_THRESH)\n",
    "            \n",
    "            for (cls_r, conf_r, x1_abs, y1_abs, x2_abs, y2_abs) in dedup_pipeline_dets:\n",
    "                final_area = (x2_abs - x1_abs) * (y2_abs - y1_abs)\n",
    "                if final_area > 30000:\n",
    "                    print(f\"Discarding pipeline detection with area {final_area} pixels (exceeds limit)\")\n",
    "                    continue\n",
    "                print(f\"  -> Final Pipeline DRONE kept: Conf={conf_r:.2f}, Box=({x1_abs}, {y1_abs}, {x2_abs}, {y2_abs})\")\n",
    "                x_center = (x1_abs + x2_abs) / 2 / original_image.shape[1]\n",
    "                y_center = (y1_abs + y2_abs) / 2 / original_image.shape[0]\n",
    "                bbox_width = (x2_abs - x1_abs) / original_image.shape[1]\n",
    "                bbox_height = (y2_abs - y1_abs) / original_image.shape[0]\n",
    "                yolo_labels.append(f\"p {cls_r} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\")\n",
    "                cv2.rectangle(original_image, (x1_abs, y1_abs), (x2_abs, y2_abs), color=(0, 255, 0), thickness=2)\n",
    "            \n",
    "            # *** Vectorized Re-addition of Non-Drone YOLO Detections ***\n",
    "            # Convert final pipeline boxes (from yolo_labels) to absolute xyxy format.\n",
    "            final_pipeline_boxes = []\n",
    "            for lbl in yolo_labels:\n",
    "                parts = lbl.strip().split()\n",
    "                # Expect either \"p ...\" or \"y ...\" or \"y2 ...\" format\n",
    "                prefix, cls_id, xc, yc, w, h = parts\n",
    "                xc, yc, w, h = map(float, (xc, yc, w, h))\n",
    "                x_min = (xc - w/2) * original_image.shape[1]\n",
    "                y_min = (yc - h/2) * original_image.shape[0]\n",
    "                x_max = (xc + w/2) * original_image.shape[1]\n",
    "                y_max = (yc + h/2) * original_image.shape[0]\n",
    "                final_pipeline_boxes.append([x_min, y_min, x_max, y_max])\n",
    "            final_pipeline_boxes = np.array(final_pipeline_boxes) if final_pipeline_boxes else np.empty((0,4))\n",
    "            \n",
    "            # Convert non_drone_dets to numpy array (only xyxy part)\n",
    "            non_drone_boxes = np.array([det[:4] for det in non_drone_dets]) if non_drone_dets else np.empty((0,4))\n",
    "            if non_drone_boxes.size > 0 and final_pipeline_boxes.size > 0:\n",
    "                iou_matrix = compute_iou_vectorized(non_drone_boxes, final_pipeline_boxes)\n",
    "                max_iou = np.max(iou_matrix, axis=1)\n",
    "            elif non_drone_boxes.size > 0:\n",
    "                max_iou = np.zeros(len(non_drone_dets))\n",
    "            else:\n",
    "                max_iou = np.array([])\n",
    "            overlap_threshold = 0.5\n",
    "            indices_to_readd = np.where(max_iou < overlap_threshold)[0]\n",
    "            for idx in indices_to_readd:\n",
    "                nx1, ny1, nx2, ny2, ncls, nconf = non_drone_dets[idx]\n",
    "                print(f\"Re-adding YOLO non-drone box (class={ncls}) with no significant overlap: ({nx1}, {ny1}, {nx2}, {ny2})\")\n",
    "                cv2.rectangle(original_image, (nx1, ny1), (nx2, ny2), color=(255, 0, 0), thickness=2)\n",
    "                x_center = (nx1 + nx2) / 2 / original_image.shape[1]\n",
    "                y_center = (ny1 + ny2) / 2 / original_image.shape[0]\n",
    "                bbox_width = (nx2 - nx1) / original_image.shape[1]\n",
    "                bbox_height = (ny2 - ny1) / original_image.shape[0]\n",
    "                yolo_labels.append(f\"y {ncls} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\")\n",
    "            \n",
    "            output_path = os.path.join(results_output_dir, os.path.basename(image_path))\n",
    "            cv2.imwrite(output_path, original_image)\n",
    "            \n",
    "            label_path = os.path.join(labels_output_dir, os.path.splitext(os.path.basename(image_path))[0] + \".txt\")\n",
    "            print(\"Final label set:\", yolo_labels)\n",
    "            with open(label_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(yolo_labels))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing anomalies for {image_path}: {e}\")\n",
    "            print(\"Saving YOLO-only results for this image...\")\n",
    "            output_path = os.path.join(results_output_dir, os.path.basename(image_path))\n",
    "            cv2.imwrite(output_path, original_image)\n",
    "            label_path = os.path.join(labels_output_dir, os.path.splitext(os.path.basename(image_path))[0] + \".txt\")\n",
    "            with open(label_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(yolo_labels))\n",
    "                \n",
    "eng.quit()\n",
    "print(\"\\nPipeline completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for txt format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### divided by drone size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: 0.002142334375 0.002587890625 0.0001879890625 0.00047851562500000005\n",
      "Normalized: 0.0025500484375 0.0020068359375 9.033281250000001e-05 7.8125e-05\n",
      "Normalized: 0.002064209375 0.0020715328125 0.00012939374999999998 0.00010009687499999999\n",
      "Normalized: 0.001923828125 0.000924071875 0.00027832031250000003 0.000290528125\n",
      "Normalized: 0.002033690625 0.0018518062500000002 0.0001220703125 0.00055908125\n",
      "Normalized: 0.002080078125 0.001291503125 0.00010253906250000001 0.00016113281249999998\n",
      "Normalized: 0.0017761234375 0.003013915625 0.000134278125 0.0002270515625\n",
      "Normalized: 0.0020617671875 0.0018078609374999998 7.08e-05 0.000163575\n",
      "Normalized: 0.0019323734374999998 0.0010632328125 0.000134278125 0.00018310625\n",
      "Normalized: 0.0021594234375 0.0016577156249999998 0.000114746875 0.0001806640625\n",
      "Normalized: 0.00287475625 0.0015148921875 0.00024658125000000003 0.00033935625\n",
      "Normalized: 0.0019934078125 0.0027038578125 7.568437500000001e-05 0.00019775312500000002\n",
      "Normalized: 0.002053221875 0.0018212890624999998 8.7890625e-05 0.00016601562499999998\n",
      "Normalized: 0.0034375000000000005 0.0014880375 0.0003173828125 0.000354003125\n",
      "Normalized: 0.0021337890625 0.00196899375 0.000302734375 0.0005883796875\n",
      "Normalized: 0.002181396875 0.0020129390625 0.000290528125 0.0005688484375\n",
      "Normalized: 0.0022119140624999997 0.0019799796875000003 9.765625e-05 0.00016113281249999998\n",
      "Normalized: 0.0021228031250000003 0.002502440625 9.52140625e-05 0.00022460937499999998\n",
      "\n",
      "Metrics for category: <=1% (Image Count = 4515)\n",
      "YOLO-Only Metrics:\n",
      "  Precision: 0.88\n",
      "  Recall: 0.40\n",
      "  F1-Score: 0.55\n",
      "  Mean IoU: 0.74\n",
      "  True Positives: 1893\n",
      "  False Positives: 246\n",
      "  False Negatives: 2803\n",
      "YOLO + HS Pipeline Metrics:\n",
      "  Precision: 0.83\n",
      "  Recall: 0.41\n",
      "  F1-Score: 0.55\n",
      "  Mean IoU: 0.74\n",
      "  True Positives: 1944\n",
      "  False Positives: 388\n",
      "  False Negatives: 2752\n",
      "\n",
      "Metrics for category: 1%-5% (Image Count = 562)\n",
      "YOLO-Only Metrics:\n",
      "  Precision: 0.91\n",
      "  Recall: 0.45\n",
      "  F1-Score: 0.61\n",
      "  Mean IoU: 0.76\n",
      "  True Positives: 449\n",
      "  False Positives: 45\n",
      "  False Negatives: 538\n",
      "YOLO + HS Pipeline Metrics:\n",
      "  Precision: 0.89\n",
      "  Recall: 0.46\n",
      "  F1-Score: 0.61\n",
      "  Mean IoU: 0.76\n",
      "  True Positives: 454\n",
      "  False Positives: 57\n",
      "  False Negatives: 533\n",
      "\n",
      "Metrics for category: >5% (Image Count = 0)\n",
      "YOLO-Only Metrics:\n",
      "  Precision: 0\n",
      "  Recall: 0\n",
      "  F1-Score: 0\n",
      "  Mean IoU: 0\n",
      "  True Positives: 0\n",
      "  False Positives: 0\n",
      "  False Negatives: 0\n",
      "YOLO + HS Pipeline Metrics:\n",
      "  Precision: 0\n",
      "  Recall: 0\n",
      "  F1-Score: 0\n",
      "  Mean IoU: 0\n",
      "  True Positives: 0\n",
      "  False Positives: 0\n",
      "  False Negatives: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def load_yolo_labels(label_folder, normalize=False, img_width=480, img_height=640):\n",
    "    \"\"\"\n",
    "    Load YOLO-format labels from a folder. Lines can be either:\n",
    "      [class_id, x_center, y_center, width, height]\n",
    "    or\n",
    "      [prefix, class_id, x_center, y_center, width, height]\n",
    "    where prefix can be \"y\", \"p\", or \"y2\".\n",
    "    If any coordinate > 1, we normalize by (img_width, img_height).\n",
    "    \n",
    "    Returns:\n",
    "      dict: { image_name: list of bounding boxes }\n",
    "            each bounding box is [prefix, class_id, x_center, y_center, width, height]\n",
    "            (if no prefix is found, prefix=None).\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    for label_file in Path(label_folder).glob(\"*.txt\"):\n",
    "        image_name = label_file.stem\n",
    "        with open(label_file, \"r\") as f:\n",
    "            bboxes = []\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "\n",
    "                if len(parts) == 5:\n",
    "                    # old style: class_id, x_center, y_center, width, height\n",
    "                    prefix = None\n",
    "                    class_id, x_center, y_center, width, height = map(float, parts)\n",
    "                elif len(parts) == 6:\n",
    "                    # new style: prefix, class_id, x_center, y_center, width, height\n",
    "                    prefix = parts[0]\n",
    "                    class_id, x_center, y_center, width, height = map(float, parts[1:])\n",
    "                else:\n",
    "                    # unexpected format\n",
    "                    print(f\"Skipping line (unexpected format): {line}\")\n",
    "                    continue\n",
    "\n",
    "                # If something is > 1, assume absolute coords => normalize\n",
    "                if any(val > 1 for val in (x_center, y_center, width, height)):\n",
    "                    x_center /= img_width\n",
    "                    y_center /= img_height\n",
    "                    width    /= img_width\n",
    "                    height   /= img_height\n",
    "                    print(\"Normalized:\", x_center, y_center, width, height)\n",
    "\n",
    "                bboxes.append([prefix, class_id, x_center, y_center, width, height])\n",
    "            labels[image_name] = bboxes\n",
    "    return labels\n",
    "\n",
    "def yolo_to_xyxy(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert from [prefix, class_id, x_center, y_center, w, h]\n",
    "    or [class_id, x_center, y_center, w, h]\n",
    "    to [x_min, y_min, x_max, y_max].\n",
    "    \"\"\"\n",
    "    if len(bbox) == 5:\n",
    "        # old: [class_id, x_center, y_center, width, height]\n",
    "        class_id, x_center, y_center, width, height = bbox\n",
    "    else:\n",
    "        # new: [prefix, class_id, x_center, y_center, width, height]\n",
    "        _, class_id, x_center, y_center, width, height = bbox\n",
    "    \n",
    "    x_min = (x_center - width / 2) * img_width\n",
    "    y_min = (y_center - height / 2) * img_height\n",
    "    x_max = (x_center + width / 2) * img_width\n",
    "    y_max = (y_center + height / 2) * img_height\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x_min = max(box1[0], box2[0])\n",
    "    y_min = max(box1[1], box2[1])\n",
    "    x_max = min(box1[2], box2[2])\n",
    "    y_max = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area_box1 + area_box2 - intersection\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def categorize_images(labels, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Categorize images based on bounding box area ratio:\n",
    "      <=1%, 1%-5%, >5%\n",
    "    Each bounding box is either [prefix, class_id, x_c, y_c, w, h]\n",
    "    or [class_id, x_c, y_c, w, h].\n",
    "    \"\"\"\n",
    "    categories = {\"<=1%\": [], \"1%-5%\": [], \">5%\": []}\n",
    "    \n",
    "    for image_name, bboxes in labels.items():\n",
    "        for bbox in bboxes:\n",
    "            if len(bbox) == 5:\n",
    "                class_id, x_c, y_c, w, h = bbox\n",
    "            else:\n",
    "                prefix, class_id, x_c, y_c, w, h = bbox\n",
    "            \n",
    "            width_pixels  = w * img_width\n",
    "            height_pixels = h * img_height\n",
    "            area_ratio = (width_pixels * height_pixels) / (img_width * img_height)\n",
    "\n",
    "            if area_ratio <= 0.01:\n",
    "                categories[\"<=1%\"].append(image_name)\n",
    "            elif 0.01 < area_ratio <= 0.05:\n",
    "                categories[\"1%-5%\"].append(image_name)\n",
    "            else:\n",
    "                categories[\">5%\"].append(image_name)\n",
    "\n",
    "    return categories\n",
    "\n",
    "def evaluate_detections(ground_truths, detections, img_width, img_height, iou_threshold=0.5, category_images=None):\n",
    "    \"\"\"\n",
    "    Evaluate detections against ground truths using precision, recall, F1 score, and mean IoU.\n",
    "    Duplicated detections that differ only by prefix (e.g., 'y' vs 'y2') are treated as identical.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    ious = []\n",
    "\n",
    "    # If we only want to evaluate a subset of images (based on category)\n",
    "    if category_images is not None:\n",
    "        filtered_ground_truths = {k: v for k, v in ground_truths.items() if k in category_images}\n",
    "        filtered_detections   = {k: v for k, v in detections.items()   if k in category_images}\n",
    "    else:\n",
    "        filtered_ground_truths = ground_truths\n",
    "        filtered_detections   = detections\n",
    "\n",
    "    for image_name, gt_bboxes in filtered_ground_truths.items():\n",
    "        pred_bboxes = filtered_detections.get(image_name, [])\n",
    "        \n",
    "        # Deduplicate predicted bboxes ignoring prefix differences\n",
    "        unique_pred_bboxes = []\n",
    "        seen = set()\n",
    "        for pred_bbox in pred_bboxes:\n",
    "            if len(pred_bbox) == 5:\n",
    "                key = tuple(pred_bbox)\n",
    "            else:\n",
    "                # key ignores the prefix (element 0)\n",
    "                key = (pred_bbox[1], pred_bbox[2], pred_bbox[3], pred_bbox[4], pred_bbox[5])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_pred_bboxes.append(pred_bbox)\n",
    "\n",
    "        matched_gt = set()\n",
    "\n",
    "        for pred_bbox in unique_pred_bboxes:\n",
    "            pred_box_xyxy = yolo_to_xyxy(pred_bbox, img_width, img_height)\n",
    "            match_found = False\n",
    "\n",
    "            for i, gt_bbox in enumerate(gt_bboxes):\n",
    "                if i in matched_gt:\n",
    "                    continue\n",
    "\n",
    "                gt_box_xyxy = yolo_to_xyxy(gt_bbox, img_width, img_height)\n",
    "                iou = calculate_iou(pred_box_xyxy, gt_box_xyxy)\n",
    "                if iou >= iou_threshold:\n",
    "                    tp += 1\n",
    "                    matched_gt.add(i)\n",
    "                    match_found = True\n",
    "                    ious.append(iou)\n",
    "                    break\n",
    "\n",
    "            if not match_found:\n",
    "                fp += 1\n",
    "\n",
    "        fn += (len(gt_bboxes) - len(matched_gt))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1        = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    mean_iou  = np.mean(ious) if ious else 0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Mean IoU\": mean_iou,\n",
    "        \"True Positives\": tp,\n",
    "        \"False Positives\": fp,\n",
    "        \"False Negatives\": fn,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    ground_truth_folder = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\train\\labels\"\n",
    "    yolo_only_folder    = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\YOLO_train_results\\predict\\labels\"\n",
    "    hs_pipeline_folder  = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\trainresults\\labels\"\n",
    "\n",
    "    img_width, img_height = 640, 640\n",
    "\n",
    "    # Load labels (assuming they're normalized or partially normalized)\n",
    "    ground_truths = load_yolo_labels(ground_truth_folder, img_width=img_width, img_height=img_height)\n",
    "    yolo_only_detections = load_yolo_labels(yolo_only_folder, img_width=img_width, img_height=img_height)\n",
    "    hs_pipeline_detections = load_yolo_labels(hs_pipeline_folder, normalize=True, img_width=img_width, img_height=img_height)\n",
    "\n",
    "    # Categorize images\n",
    "    categories = categorize_images(ground_truths, img_width, img_height)\n",
    "\n",
    "    # Evaluate each category\n",
    "    for category, image_list in categories.items():\n",
    "        print(f\"\\nMetrics for category: {category} (Image Count = {len(image_list)})\")\n",
    "\n",
    "        # Evaluate YOLO-only\n",
    "        yolo_only_metrics = evaluate_detections(\n",
    "            ground_truths, \n",
    "            yolo_only_detections, \n",
    "            img_width, \n",
    "            img_height, \n",
    "            category_images=image_list\n",
    "        )\n",
    "        print(\"YOLO-Only Metrics:\")\n",
    "        for metric, value in yolo_only_metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {metric}: {value}\")\n",
    "\n",
    "        # Evaluate YOLO + HS pipeline (which may include boxes with prefix 'y2')\n",
    "        hs_pipeline_metrics = evaluate_detections(\n",
    "            ground_truths, \n",
    "            hs_pipeline_detections, \n",
    "            img_width, \n",
    "            img_height, \n",
    "            category_images=image_list\n",
    "        )\n",
    "        print(\"YOLO + HS Pipeline Metrics:\")\n",
    "        for metric, value in hs_pipeline_metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: 0.002142334375 0.002587890625 0.0001879890625 0.00047851562500000005\n",
      "Normalized: 0.0025500484375 0.0020068359375 9.033281250000001e-05 7.8125e-05\n",
      "Normalized: 0.002064209375 0.0020715328125 0.00012939374999999998 0.00010009687499999999\n",
      "Normalized: 0.001923828125 0.000924071875 0.00027832031250000003 0.000290528125\n",
      "Normalized: 0.002033690625 0.0018518062500000002 0.0001220703125 0.00055908125\n",
      "Normalized: 0.002080078125 0.001291503125 0.00010253906250000001 0.00016113281249999998\n",
      "Normalized: 0.0017761234375 0.003013915625 0.000134278125 0.0002270515625\n",
      "Normalized: 0.0020617671875 0.0018078609374999998 7.08e-05 0.000163575\n",
      "Normalized: 0.0019323734374999998 0.0010632328125 0.000134278125 0.00018310625\n",
      "Normalized: 0.0021594234375 0.0016577156249999998 0.000114746875 0.0001806640625\n",
      "Normalized: 0.00287475625 0.0015148921875 0.00024658125000000003 0.00033935625\n",
      "Normalized: 0.0019934078125 0.0027038578125 7.568437500000001e-05 0.00019775312500000002\n",
      "Normalized: 0.002053221875 0.0018212890624999998 8.7890625e-05 0.00016601562499999998\n",
      "Normalized: 0.0034375000000000005 0.0014880375 0.0003173828125 0.000354003125\n",
      "Normalized: 0.0021337890625 0.00196899375 0.000302734375 0.0005883796875\n",
      "Normalized: 0.002181396875 0.0020129390625 0.000290528125 0.0005688484375\n",
      "Normalized: 0.0022119140624999997 0.0019799796875000003 9.765625e-05 0.00016113281249999998\n",
      "Normalized: 0.0021228031250000003 0.002502440625 9.52140625e-05 0.00022460937499999998\n",
      "Overall YOLO-Only Metrics:\n",
      "  Precision: 0.89\n",
      "  Recall: 0.43\n",
      "  F1-Score: 0.58\n",
      "  Mean IoU: 0.75\n",
      "  True Positives: 2203\n",
      "  False Positives: 280\n",
      "  False Negatives: 2874\n",
      "\n",
      "Overall YOLO + HS Pipeline Metrics:\n",
      "  Precision: 0.84\n",
      "  Recall: 0.44\n",
      "  F1-Score: 0.58\n",
      "  Mean IoU: 0.74\n",
      "  True Positives: 2257\n",
      "  False Positives: 428\n",
      "  False Negatives: 2820\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set the folder paths for the labels (all in YOLO format).\n",
    "    ground_truth_folder = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\train\\labels\"\n",
    "    yolo_only_folder    = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\YOLO_train_results\\predict\\labels\"\n",
    "    hs_pipeline_folder  = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\trainresults\\labels\"\n",
    "\n",
    "    # Set image dimensions (should match your data)\n",
    "    img_width, img_height = 640, 640\n",
    "\n",
    "    # Load YOLO-format labels for ground truth and detections.\n",
    "    ground_truths = load_yolo_labels(ground_truth_folder, img_width=img_width, img_height=img_height)\n",
    "    yolo_only_detections = load_yolo_labels(yolo_only_folder, img_width=img_width, img_height=img_height)\n",
    "    hs_pipeline_detections = load_yolo_labels(hs_pipeline_folder, normalize=True, img_width=img_width, img_height=img_height)\n",
    "\n",
    "    # Compute overall evaluation metrics without filtering by image size category.\n",
    "    overall_yolo_only_metrics = evaluate_detections(\n",
    "        ground_truths,\n",
    "        yolo_only_detections,\n",
    "        img_width,\n",
    "        img_height\n",
    "    )\n",
    "    overall_hs_pipeline_metrics = evaluate_detections(\n",
    "        ground_truths,\n",
    "        hs_pipeline_detections,\n",
    "        img_width,\n",
    "        img_height\n",
    "    )\n",
    "\n",
    "    # Print overall results for YOLO-only detections.\n",
    "    print(\"Overall YOLO-Only Metrics:\")\n",
    "    for metric, value in overall_yolo_only_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "\n",
    "    # Print overall results for YOLO + HS pipeline detections.\n",
    "    print(\"\\nOverall YOLO + HS Pipeline Metrics:\")\n",
    "    for metric, value in overall_hs_pipeline_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### results for drones only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: 0.002142334375 0.002587890625 0.0001879890625 0.00047851562500000005\n",
      "Normalized: 0.0025500484375 0.0020068359375 9.033281250000001e-05 7.8125e-05\n",
      "Normalized: 0.002064209375 0.0020715328125 0.00012939374999999998 0.00010009687499999999\n",
      "Normalized: 0.001923828125 0.000924071875 0.00027832031250000003 0.000290528125\n",
      "Normalized: 0.002033690625 0.0018518062500000002 0.0001220703125 0.00055908125\n",
      "Normalized: 0.002080078125 0.001291503125 0.00010253906250000001 0.00016113281249999998\n",
      "Normalized: 0.0017761234375 0.003013915625 0.000134278125 0.0002270515625\n",
      "Normalized: 0.0020617671875 0.0018078609374999998 7.08e-05 0.000163575\n",
      "Normalized: 0.0019323734374999998 0.0010632328125 0.000134278125 0.00018310625\n",
      "Normalized: 0.0021594234375 0.0016577156249999998 0.000114746875 0.0001806640625\n",
      "Normalized: 0.00287475625 0.0015148921875 0.00024658125000000003 0.00033935625\n",
      "Normalized: 0.0019934078125 0.0027038578125 7.568437500000001e-05 0.00019775312500000002\n",
      "Normalized: 0.002053221875 0.0018212890624999998 8.7890625e-05 0.00016601562499999998\n",
      "Normalized: 0.0034375000000000005 0.0014880375 0.0003173828125 0.000354003125\n",
      "Normalized: 0.0021337890625 0.00196899375 0.000302734375 0.0005883796875\n",
      "Normalized: 0.002181396875 0.0020129390625 0.000290528125 0.0005688484375\n",
      "Normalized: 0.0022119140624999997 0.0019799796875000003 9.765625e-05 0.00016113281249999998\n",
      "Normalized: 0.0021228031250000003 0.002502440625 9.52140625e-05 0.00022460937499999998\n",
      "\n",
      "Metrics for category: <=1% (Image Count = 2087)\n",
      "YOLO-Only (Drones) Metrics:\n",
      "  Precision: 0.91\n",
      "  Recall: 0.82\n",
      "  F1-Score: 0.86\n",
      "  Mean IoU: 0.75\n",
      "  True Positives: 1770\n",
      "  False Positives: 185\n",
      "  False Negatives: 379\n",
      "  mAP: 0.91\n",
      "YOLO + HS Pipeline (Drones) Metrics:\n",
      "  Precision: 0.85\n",
      "  Recall: 0.85\n",
      "  F1-Score: 0.85\n",
      "  Mean IoU: 0.74\n",
      "  True Positives: 1822\n",
      "  False Positives: 311\n",
      "  False Negatives: 327\n",
      "  mAP: 0.85\n",
      "\n",
      "Metrics for category: 1%-5% (Image Count = 360)\n",
      "YOLO-Only (Drones) Metrics:\n",
      "  Precision: 0.95\n",
      "  Recall: 0.90\n",
      "  F1-Score: 0.92\n",
      "  Mean IoU: 0.77\n",
      "  True Positives: 385\n",
      "  False Positives: 22\n",
      "  False Negatives: 44\n",
      "  mAP: 0.95\n",
      "YOLO + HS Pipeline (Drones) Metrics:\n",
      "  Precision: 0.92\n",
      "  Recall: 0.91\n",
      "  F1-Score: 0.91\n",
      "  Mean IoU: 0.76\n",
      "  True Positives: 392\n",
      "  False Positives: 36\n",
      "  False Negatives: 37\n",
      "  mAP: 0.92\n",
      "\n",
      "Metrics for category: >5% (Image Count = 0)\n",
      "YOLO-Only (Drones) Metrics:\n",
      "  Precision: 0\n",
      "  Recall: 0\n",
      "  F1-Score: 0\n",
      "  Mean IoU: 0\n",
      "  True Positives: 0\n",
      "  False Positives: 0\n",
      "  False Negatives: 0\n",
      "  mAP: 0.00\n",
      "YOLO + HS Pipeline (Drones) Metrics:\n",
      "  Precision: 0\n",
      "  Recall: 0\n",
      "  F1-Score: 0\n",
      "  Mean IoU: 0\n",
      "  True Positives: 0\n",
      "  False Positives: 0\n",
      "  False Negatives: 0\n",
      "  mAP: 0.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def load_yolo_labels(label_folder, normalize=False, img_width=480, img_height=640):\n",
    "    \"\"\"\n",
    "    Load YOLO-format labels from a folder. Expected line formats:\n",
    "      - 5 tokens: [class_id, x_center, y_center, width, height]\n",
    "      - 6 tokens: either [prefix, class_id, x_center, y_center, width, height] or [class_id, confidence, x_center, y_center, width, height]\n",
    "      - 7 tokens: [prefix, class_id, confidence, x_center, y_center, width, height]\n",
    "    If any coordinate > 1, we assume they are absolute and normalize by (img_width, img_height).\n",
    "    Returns:\n",
    "      dict: { image_name: list of bounding boxes }\n",
    "            Each bounding box is stored as a list:\n",
    "              [prefix, class_id, confidence, x_center, y_center, width, height]\n",
    "            If no prefix is present, prefix is set to None.\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    for label_file in Path(label_folder).glob(\"*.txt\"):\n",
    "        image_name = label_file.stem\n",
    "        with open(label_file, \"r\") as f:\n",
    "            bboxes = []\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "                if len(parts) == 5:\n",
    "                    # Old style: [class_id, x_center, y_center, width, height]\n",
    "                    prefix = None\n",
    "                    class_id, x_center, y_center, width, height = map(float, parts)\n",
    "                    conf = 1.0\n",
    "                elif len(parts) == 6:\n",
    "                    # Could be either:\n",
    "                    #   a) [prefix, class_id, x_center, y_center, width, height]\n",
    "                    #   b) [class_id, confidence, x_center, y_center, width, height]\n",
    "                    if not is_number(parts[0]):\n",
    "                        # Treat first token as prefix\n",
    "                        prefix = parts[0]\n",
    "                        class_id, x_center, y_center, width, height = map(float, parts[1:])\n",
    "                        conf = 1.0\n",
    "                    else:\n",
    "                        prefix = None\n",
    "                        class_id, conf, x_center, y_center, width, height = map(float, parts)\n",
    "                elif len(parts) == 7:\n",
    "                    # New style: [prefix, class_id, confidence, x_center, y_center, width, height]\n",
    "                    prefix = parts[0]\n",
    "                    class_id, conf, x_center, y_center, width, height = map(float, parts[1:])\n",
    "                else:\n",
    "                    print(f\"Skipping line (unexpected format): {line}\")\n",
    "                    continue\n",
    "\n",
    "                # Replace prefix 'y2' with 'y'\n",
    "                if prefix == \"y2\":\n",
    "                    prefix = \"y\"\n",
    "\n",
    "                # If any coordinate is greater than 1, assume absolute values and normalize them.\n",
    "                if any(val > 1 for val in (x_center, y_center, width, height)):\n",
    "                    x_center /= img_width\n",
    "                    y_center /= img_height\n",
    "                    width    /= img_width\n",
    "                    height   /= img_height\n",
    "                    print(\"Normalized:\", x_center, y_center, width, height)\n",
    "\n",
    "                bboxes.append([prefix, class_id, conf, x_center, y_center, width, height])\n",
    "            labels[image_name] = bboxes\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def filter_to_drones_only(labels, prefixes=('y', 'y2', 'p', None)):\n",
    "    \"\"\"\n",
    "    Given a dict of {image_name: [ [prefix, class_id, conf, x_center, y_center, w, h], ... ]},\n",
    "    return a new dict that keeps only bounding boxes where class_id == 0\n",
    "    and prefix is in the allowed list of prefixes ('y', 'y2', 'p', or None).\n",
    "    \"\"\"\n",
    "    filtered = {}\n",
    "    for image_name, bboxes in labels.items():\n",
    "        drone_bboxes = []\n",
    "        for bb in bboxes:\n",
    "            prefix, class_id = bb[0], bb[1]\n",
    "            if int(class_id) == 0 and prefix in prefixes:\n",
    "                drone_bboxes.append(bb)\n",
    "        if drone_bboxes:\n",
    "            filtered[image_name] = drone_bboxes\n",
    "    return filtered\n",
    "\n",
    "def yolo_to_xyxy(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert a bounding box from YOLO format to [x_min, y_min, x_max, y_max].\n",
    "    Supports three formats:\n",
    "      - [class_id, x_center, y_center, width, height] (len 5)\n",
    "      - [prefix, class_id, x_center, y_center, width, height] (len 6)\n",
    "      - [prefix, class_id, confidence, x_center, y_center, width, height] (len 7)\n",
    "    \"\"\"\n",
    "    if len(bbox) == 5:\n",
    "        class_id, x_center, y_center, width, height = bbox\n",
    "    elif len(bbox) == 6:\n",
    "        # Assume format: [prefix, class_id, x_center, y_center, width, height]\n",
    "        _, class_id, x_center, y_center, width, height = bbox\n",
    "    elif len(bbox) == 7:\n",
    "        # Format: [prefix, class_id, confidence, x_center, y_center, width, height]\n",
    "        _, class_id, _, x_center, y_center, width, height = bbox\n",
    "    x_min = (x_center - width / 2) * img_width\n",
    "    y_min = (y_center - height / 2) * img_height\n",
    "    x_max = (x_center + width / 2) * img_width\n",
    "    y_max = (y_center + height / 2) * img_height\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x_min = max(box1[0], box2[0])\n",
    "    y_min = max(box1[1], box2[1])\n",
    "    x_max = min(box1[2], box2[2])\n",
    "    y_max = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area_box1 + area_box2 - intersection\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def categorize_images(labels, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Categorize images based on bounding box area ratio:\n",
    "      <=1%, 1%-5%, >5%\n",
    "    \"\"\"\n",
    "    categories = {\"<=1%\": [], \"1%-5%\": [], \">5%\": []}\n",
    "    \n",
    "    for image_name, bboxes in labels.items():\n",
    "        for bbox in bboxes:\n",
    "            # bbox format: [prefix, class_id, conf, x_center, y_center, w, h]\n",
    "            _, _, _, x_c, y_c, w, h = bbox\n",
    "            width_pixels  = w * img_width\n",
    "            height_pixels = h * img_height\n",
    "            area_ratio = (width_pixels * height_pixels) / (img_width * img_height)\n",
    "\n",
    "            if area_ratio <= 0.01:\n",
    "                categories[\"<=1%\"].append(image_name)\n",
    "            elif 0.01 < area_ratio <= 0.05:\n",
    "                categories[\"1%-5%\"].append(image_name)\n",
    "            else:\n",
    "                categories[\">5%\"].append(image_name)\n",
    "\n",
    "    return categories\n",
    "\n",
    "def evaluate_detections(ground_truths, detections, img_width, img_height, iou_threshold=0.5, category_images=None):\n",
    "    \"\"\"\n",
    "    Evaluate detections by computing Precision, Recall, F1-Score, Mean IoU, and counts of TP, FP, FN.\n",
    "    Both ground_truths and detections should have bounding boxes in the format:\n",
    "      [prefix, class_id, confidence, x_center, y_center, width, height]\n",
    "    For ground truths (which generally lack a confidence value), confidence is assumed to be 1.0.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    ious = []\n",
    "\n",
    "    if category_images is not None:\n",
    "        filtered_ground_truths = {k: v for k, v in ground_truths.items() if k in category_images}\n",
    "        filtered_detections   = {k: v for k, v in detections.items()   if k in category_images}\n",
    "    else:\n",
    "        filtered_ground_truths = ground_truths\n",
    "        filtered_detections   = detections\n",
    "\n",
    "    for image_name, gt_bboxes in filtered_ground_truths.items():\n",
    "        pred_bboxes = filtered_detections.get(image_name, [])\n",
    "        \n",
    "        # Deduplicate predicted bboxes (naive check)\n",
    "        unique_pred_bboxes = []\n",
    "        for pred_bbox in pred_bboxes:\n",
    "            if pred_bbox not in unique_pred_bboxes:\n",
    "                unique_pred_bboxes.append(pred_bbox)\n",
    "\n",
    "        matched_gt = set()\n",
    "\n",
    "        for pred_bbox in unique_pred_bboxes:\n",
    "            pred_box_xyxy = yolo_to_xyxy(pred_bbox, img_width, img_height)\n",
    "            match_found = False\n",
    "\n",
    "            for i, gt_bbox in enumerate(gt_bboxes):\n",
    "                if i in matched_gt:\n",
    "                    continue\n",
    "\n",
    "                gt_box_xyxy = yolo_to_xyxy(gt_bbox, img_width, img_height)\n",
    "                iou = calculate_iou(pred_box_xyxy, gt_box_xyxy)\n",
    "                if iou >= iou_threshold:\n",
    "                    tp += 1\n",
    "                    matched_gt.add(i)\n",
    "                    match_found = True\n",
    "                    ious.append(iou)\n",
    "                    break\n",
    "\n",
    "            if not match_found:\n",
    "                fp += 1\n",
    "\n",
    "        fn += (len(gt_bboxes) - len(matched_gt))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1        = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    mean_iou  = np.mean(ious) if ious else 0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Mean IoU\": mean_iou,\n",
    "        \"True Positives\": tp,\n",
    "        \"False Positives\": fp,\n",
    "        \"False Negatives\": fn,\n",
    "    }\n",
    "\n",
    "def compute_average_precision(ground_truths, detections, img_width, img_height, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute average precision (AP) for the detections against the ground truths.\n",
    "    For each image, predictions are sorted by their confidence scores.\n",
    "    A prediction is considered a true positive if its IoU with any unmatched ground truth exceeds the threshold.\n",
    "    AP is then calculated using scikit-learn's average_precision_score.\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "\n",
    "    for image_name, gt_bboxes in ground_truths.items():\n",
    "        pred_bboxes = detections.get(image_name, [])\n",
    "        # Sort predictions by confidence descending.\n",
    "        sorted_preds = sorted(pred_bboxes, key=lambda b: b[2] if (len(b) >= 7 or (len(b)==6 and b[0].isalpha())) else 1.0, reverse=True)\n",
    "        detected = [False] * len(gt_bboxes)\n",
    "\n",
    "        for pred in sorted_preds:\n",
    "            pred_box = yolo_to_xyxy(pred, img_width, img_height)\n",
    "            # If confidence is provided, use it; otherwise default to 1.0.\n",
    "            score = pred[2] if (len(pred) >= 7 or (len(pred)==6 and pred[0].isalpha())) else 1.0\n",
    "            match_found = False\n",
    "\n",
    "            for i, gt in enumerate(gt_bboxes):\n",
    "                if detected[i]:\n",
    "                    continue\n",
    "                gt_box = yolo_to_xyxy(gt, img_width, img_height)\n",
    "                iou = calculate_iou(pred_box, gt_box)\n",
    "                if iou >= iou_threshold:\n",
    "                    match_found = True\n",
    "                    detected[i] = True\n",
    "                    break\n",
    "\n",
    "            all_scores.append(score)\n",
    "            all_labels.append(1 if match_found else 0)\n",
    "\n",
    "    if not all_labels:\n",
    "        return 0\n",
    "    ap = average_precision_score(np.array(all_labels), np.array(all_scores))\n",
    "    return ap\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define folders for ground truth, YOLO-only predictions, and YOLO+HS pipeline predictions.\n",
    "    ground_truth_folder = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\train\\labels\"\n",
    "    yolo_only_folder    = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\YOLO_train_results\\predict\\labels\"\n",
    "    hs_pipeline_folder  = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\trainresults\\labels\"\n",
    "\n",
    "    # Adjust image dimensions if necessary.\n",
    "    img_width, img_height = 640, 640\n",
    "\n",
    "    # 1) Load all bounding boxes\n",
    "    ground_truths_all = load_yolo_labels(ground_truth_folder, img_width=img_width, img_height=img_height)\n",
    "    yolo_only_all     = load_yolo_labels(yolo_only_folder,    img_width=img_width, img_height=img_height)\n",
    "    hs_pipeline_all   = load_yolo_labels(hs_pipeline_folder,  normalize=True, \n",
    "                                         img_width=img_width, img_height=img_height)\n",
    "\n",
    "    # 2) Filter to keep only drone detections (class_id == 0)\n",
    "    ground_truths_drones = filter_to_drones_only(ground_truths_all, prefixes=('y', 'y2', None))\n",
    "    yolo_only_drones     = filter_to_drones_only(yolo_only_all, prefixes=('y', 'y2', None))\n",
    "    hs_pipeline_drones   = filter_to_drones_only(hs_pipeline_all, prefixes=('y', 'y2', 'p', None))\n",
    "\n",
    "    # 3) Categorize images by the area of the drone bounding boxes.\n",
    "    categories = categorize_images(ground_truths_drones, img_width, img_height)\n",
    "\n",
    "    # 4) Evaluate detections and compute mAP for each category.\n",
    "    for category, image_list in categories.items():\n",
    "        print(f\"\\nMetrics for category: {category} (Image Count = {len(image_list)})\")\n",
    "\n",
    "        # YOLO-only metrics\n",
    "        yolo_only_metrics = evaluate_detections(\n",
    "            ground_truths_drones,\n",
    "            yolo_only_drones,\n",
    "            img_width,\n",
    "            img_height,\n",
    "            category_images=image_list\n",
    "        )\n",
    "        mAP_yolo = compute_average_precision(\n",
    "            {k: v for k, v in ground_truths_drones.items() if k in image_list},\n",
    "            {k: v for k, v in yolo_only_drones.items() if k in image_list},\n",
    "            img_width,\n",
    "            img_height\n",
    "        )\n",
    "        print(\"YOLO-Only (Drones) Metrics:\")\n",
    "        for metric, value in yolo_only_metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {metric}: {value}\")\n",
    "        print(f\"  mAP: {mAP_yolo:.2f}\")\n",
    "\n",
    "        # YOLO + HS Pipeline metrics\n",
    "        hs_pipeline_metrics = evaluate_detections(\n",
    "            ground_truths_drones,\n",
    "            hs_pipeline_drones,\n",
    "            img_width,\n",
    "            img_height,\n",
    "            category_images=image_list\n",
    "        )\n",
    "        mAP_pipeline = compute_average_precision(\n",
    "            {k: v for k, v in ground_truths_drones.items() if k in image_list},\n",
    "            {k: v for k, v in hs_pipeline_drones.items() if k in image_list},\n",
    "            img_width,\n",
    "            img_height\n",
    "        )\n",
    "        print(\"YOLO + HS Pipeline (Drones) Metrics:\")\n",
    "        for metric, value in hs_pipeline_metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {metric}: {value}\")\n",
    "        print(f\"  mAP: {mAP_pipeline:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drone only overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: 0.002142334375 0.002587890625 0.0001879890625 0.00047851562500000005\n",
      "Normalized: 0.0025500484375 0.0020068359375 9.033281250000001e-05 7.8125e-05\n",
      "Normalized: 0.002064209375 0.0020715328125 0.00012939374999999998 0.00010009687499999999\n",
      "Normalized: 0.001923828125 0.000924071875 0.00027832031250000003 0.000290528125\n",
      "Normalized: 0.002033690625 0.0018518062500000002 0.0001220703125 0.00055908125\n",
      "Normalized: 0.002080078125 0.001291503125 0.00010253906250000001 0.00016113281249999998\n",
      "Normalized: 0.0017761234375 0.003013915625 0.000134278125 0.0002270515625\n",
      "Normalized: 0.0020617671875 0.0018078609374999998 7.08e-05 0.000163575\n",
      "Normalized: 0.0019323734374999998 0.0010632328125 0.000134278125 0.00018310625\n",
      "Normalized: 0.0021594234375 0.0016577156249999998 0.000114746875 0.0001806640625\n",
      "Normalized: 0.00287475625 0.0015148921875 0.00024658125000000003 0.00033935625\n",
      "Normalized: 0.0019934078125 0.0027038578125 7.568437500000001e-05 0.00019775312500000002\n",
      "Normalized: 0.002053221875 0.0018212890624999998 8.7890625e-05 0.00016601562499999998\n",
      "Normalized: 0.0034375000000000005 0.0014880375 0.0003173828125 0.000354003125\n",
      "Normalized: 0.0021337890625 0.00196899375 0.000302734375 0.0005883796875\n",
      "Normalized: 0.002181396875 0.0020129390625 0.000290528125 0.0005688484375\n",
      "Normalized: 0.0022119140624999997 0.0019799796875000003 9.765625e-05 0.00016113281249999998\n",
      "Normalized: 0.0021228031250000003 0.002502440625 9.52140625e-05 0.00022460937499999998\n",
      "Overall YOLO-Only (Drones) Metrics:\n",
      "  Precision: 0.91\n",
      "  Recall: 0.83\n",
      "  F1-Score: 0.87\n",
      "  Mean IoU: 0.75\n",
      "  True Positives: 2034\n",
      "  False Positives: 201\n",
      "  False Negatives: 413\n",
      "  mAP: 0.91\n",
      "\n",
      "Overall YOLO + HS Pipeline (Drones) Metrics:\n",
      "  Precision: 0.86\n",
      "  Recall: 0.85\n",
      "  F1-Score: 0.86\n",
      "  Mean IoU: 0.75\n",
      "  True Positives: 2092\n",
      "  False Positives: 338\n",
      "  False Negatives: 355\n",
      "  mAP: 0.86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define folders for ground truth and predictions (YOLO-only and YOLO+HS Pipeline).\n",
    "ground_truth_folder = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\train\\labels\"\n",
    "yolo_only_folder    = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\YOLO_train_results\\predict\\labels\"\n",
    "hs_pipeline_folder  = r\"C:\\Users\\michael\\Desktop\\f_proj\\birdrone_research_data\\trainresults\\labels\"\n",
    "\n",
    "# Set image dimensions.\n",
    "img_width, img_height = 640, 640\n",
    "\n",
    "# 1) Load all bounding boxes from YOLO-format labels.\n",
    "ground_truths_all = load_yolo_labels(\n",
    "    ground_truth_folder,\n",
    "    img_width=img_width, \n",
    "    img_height=img_height\n",
    ")\n",
    "yolo_only_all = load_yolo_labels(\n",
    "    yolo_only_folder,\n",
    "    img_width=img_width, \n",
    "    img_height=img_height\n",
    ")\n",
    "hs_pipeline_all = load_yolo_labels(\n",
    "    hs_pipeline_folder,\n",
    "    normalize=True,\n",
    "    img_width=img_width,\n",
    "    img_height=img_height\n",
    ")\n",
    "\n",
    "# 2) Filter to keep only drone detections (class_id == 0). This removes non-drone (e.g., birds) boxes.\n",
    "ground_truths_drones = filter_to_drones_only(ground_truths_all, prefixes=('y', 'y2', None))\n",
    "yolo_only_drones     = filter_to_drones_only(yolo_only_all, prefixes=('y', 'y2', None))\n",
    "hs_pipeline_drones   = filter_to_drones_only(hs_pipeline_all, prefixes=('y', 'y2', 'p', None))\n",
    "\n",
    "# 3) Evaluate overall metrics (across all images) for YOLO-only detections.\n",
    "overall_yolo_metrics = evaluate_detections(\n",
    "    ground_truths_drones,\n",
    "    yolo_only_drones,\n",
    "    img_width,\n",
    "    img_height\n",
    ")\n",
    "overall_yolo_mAP = compute_average_precision(\n",
    "    ground_truths_drones,\n",
    "    yolo_only_drones,\n",
    "    img_width,\n",
    "    img_height\n",
    ")\n",
    "\n",
    "print(\"Overall YOLO-Only (Drones) Metrics:\")\n",
    "for metric, value in overall_yolo_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")\n",
    "print(f\"  mAP: {overall_yolo_mAP:.2f}\")\n",
    "\n",
    "# 4) Evaluate overall metrics for YOLO + HS Pipeline detections.\n",
    "overall_pipeline_metrics = evaluate_detections(\n",
    "    ground_truths_drones,\n",
    "    hs_pipeline_drones,\n",
    "    img_width,\n",
    "    img_height\n",
    ")\n",
    "overall_pipeline_mAP = compute_average_precision(\n",
    "    ground_truths_drones,\n",
    "    hs_pipeline_drones,\n",
    "    img_width,\n",
    "    img_height\n",
    ")\n",
    "\n",
    "print(\"\\nOverall YOLO + HS Pipeline (Drones) Metrics:\")\n",
    "for metric, value in overall_pipeline_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")\n",
    "print(f\"  mAP: {overall_pipeline_mAP:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by drone size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: 0.00115966875 0.0041688375 9.765625e-05 0.0002907979166666667\n",
      "Normalized: 0.00194091875 0.0028363708333333333 0.000263671875 0.0004904520833333333\n",
      "Normalized: 0.0018359375000000001 0.0022482645833333333 0.0001171875 0.0003385416666666667\n",
      "\n",
      "Metrics for category: <=1% (Image Count = 183)\n",
      "YOLO-Only:\n",
      "  Precision = 0.71 [0.63, 0.78]\n",
      "  Recall    = 0.54 [0.46, 0.61]\n",
      "  F1-Score  = 0.61\n",
      "  Mean IoU  = 0.69\n",
      "  TP = 99, FP = 40, FN = 86\n",
      "YOLO + HS Pipeline:\n",
      "  Precision = 0.64 [0.56, 0.71]\n",
      "  Recall    = 0.58 [0.51, 0.65]\n",
      "  F1-Score  = 0.61\n",
      "  Mean IoU  = 0.69\n",
      "  TP = 107, FP = 61, FN = 78\n",
      "\n",
      "Metrics for category: 1%-5% (Image Count = 154)\n",
      "YOLO-Only:\n",
      "  Precision = 0.91 [0.85, 0.95]\n",
      "  Recall    = 0.71 [0.63, 0.77]\n",
      "  F1-Score  = 0.80\n",
      "  Mean IoU  = 0.78\n",
      "  TP = 112, FP = 11, FN = 46\n",
      "YOLO + HS Pipeline:\n",
      "  Precision = 0.90 [0.84, 0.94]\n",
      "  Recall    = 0.77 [0.70, 0.83]\n",
      "  F1-Score  = 0.83\n",
      "  Mean IoU  = 0.78\n",
      "  TP = 122, FP = 13, FN = 36\n",
      "\n",
      "Metrics for category: >5% (Image Count = 150)\n",
      "YOLO-Only:\n",
      "  Precision = 0.94 [0.89, 0.97]\n",
      "  Recall    = 0.84 [0.77, 0.89]\n",
      "  F1-Score  = 0.89\n",
      "  Mean IoU  = 0.82\n",
      "  TP = 128, FP = 8, FN = 25\n",
      "YOLO + HS Pipeline:\n",
      "  Precision = 0.89 [0.82, 0.93]\n",
      "  Recall    = 0.86 [0.79, 0.90]\n",
      "  F1-Score  = 0.87\n",
      "  Mean IoU  = 0.82\n",
      "  TP = 131, FP = 17, FN = 22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "def load_xml_ground_truths(xml_folder, img_width, img_height, class_mapping=None):\n",
    "    \"\"\"\n",
    "    Load ground truth annotations stored in XML files (e.g., Pascal VOC format).\n",
    "    \n",
    "    Each XML file is expected to contain one or more <object> elements with:\n",
    "      <name> for the class (e.g., \"drone\")\n",
    "      <bndbox> containing: <xmin>, <ymin>, <xmax>, <ymax>\n",
    "    \n",
    "    The bounding box is converted to YOLO format (normalized):\n",
    "      x_center = (xmin + xmax) / 2 / img_width\n",
    "      y_center = (ymin + ymax) / 2 / img_height\n",
    "      width    = (xmax - xmin) / img_width\n",
    "      height   = (ymax - ymin) / img_height\n",
    "    \n",
    "    Each returned bounding box is formatted as:\n",
    "      [prefix, class_id, x_center, y_center, width, height]\n",
    "    where prefix is set to None (as ground truth typically does not use a prefix).\n",
    "    \n",
    "    Args:\n",
    "      xml_folder (str): Folder containing the XML annotation files.\n",
    "      img_width (int): Width of the image in pixels.\n",
    "      img_height (int): Height of the image in pixels.\n",
    "      class_mapping (dict): Dictionary mapping object names (strings) to class_ids (ints).\n",
    "                            For example: {\"drone\": 0}. If None, defaults to {\"drone\": 0}.\n",
    "    \n",
    "    Returns:\n",
    "      dict: { image_name: list of bounding boxes } where image_name is the XML file stem.\n",
    "    \"\"\"\n",
    "    if class_mapping is None:\n",
    "        class_mapping = {\"drone\": 0}\n",
    "    \n",
    "    labels = {}\n",
    "    for xml_file in Path(xml_folder).glob(\"*.xml\"):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        image_name = xml_file.stem  # you can also get filename from XML if desired\n",
    "        \n",
    "        bboxes = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text.strip()\n",
    "            if name not in class_mapping:\n",
    "                # Skip objects not in our mapping\n",
    "                continue\n",
    "            class_id = class_mapping[name]\n",
    "            \n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bndbox.find(\"xmin\").text)\n",
    "            ymin = float(bndbox.find(\"ymin\").text)\n",
    "            xmax = float(bndbox.find(\"xmax\").text)\n",
    "            ymax = float(bndbox.find(\"ymax\").text)\n",
    "            \n",
    "            # Convert to normalized YOLO format\n",
    "            x_center = ((xmin + xmax) / 2) / img_width\n",
    "            y_center = ((ymin + ymax) / 2) / img_height\n",
    "            width = (xmax - xmin) / img_width\n",
    "            height = (ymax - ymin) / img_height\n",
    "            \n",
    "            # ground truths do not have a prefix; we set it to None\n",
    "            bboxes.append([None, class_id, x_center, y_center, width, height])\n",
    "        \n",
    "        labels[image_name] = bboxes\n",
    "    return labels\n",
    "\n",
    "def load_yolo_labels(label_folder, normalize=False, img_width=480, img_height=640):\n",
    "    \"\"\"\n",
    "    Load YOLO-format labels from a folder. Lines can be either:\n",
    "      [class_id, x_center, y_center, width, height]\n",
    "    or\n",
    "      [prefix, class_id, x_center, y_center, width, height]\n",
    "    where prefix can be \"y\", \"p\", or \"y2\".\n",
    "    If any coordinate > 1, we normalize by (img_width, img_height).\n",
    "    \n",
    "    Returns:\n",
    "      dict: { image_name: list of bounding boxes }\n",
    "            each bounding box is [prefix, class_id, x_center, y_center, width, height]\n",
    "            (if no prefix is found, prefix=None).\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    for label_file in Path(label_folder).glob(\"*.txt\"):\n",
    "        image_name = label_file.stem\n",
    "        with open(label_file, \"r\") as f:\n",
    "            bboxes = []\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "\n",
    "                if len(parts) == 5:\n",
    "                    # old style: class_id, x_center, y_center, width, height\n",
    "                    prefix = None\n",
    "                    class_id, x_center, y_center, width, height = map(float, parts)\n",
    "                elif len(parts) == 6:\n",
    "                    # new style: prefix, class_id, x_center, y_center, width, height\n",
    "                    prefix = parts[0]\n",
    "                    class_id, x_center, y_center, width, height = map(float, parts[1:])\n",
    "                else:\n",
    "                    # unexpected format\n",
    "                    print(f\"Skipping line (unexpected format): {line}\")\n",
    "                    continue\n",
    "\n",
    "                # If any coordinate is > 1, assume absolute coords => normalize\n",
    "                if any(val > 1 for val in (x_center, y_center, width, height)):\n",
    "                    x_center /= img_width\n",
    "                    y_center /= img_height\n",
    "                    width    /= img_width\n",
    "                    height   /= img_height\n",
    "                    print(\"Normalized:\", x_center, y_center, width, height)\n",
    "\n",
    "                bboxes.append([prefix, class_id, x_center, y_center, width, height])\n",
    "            labels[image_name] = bboxes\n",
    "    return labels\n",
    "\n",
    "def yolo_to_xyxy(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert from [prefix, class_id, x_center, y_center, w, h]\n",
    "    or [class_id, x_center, y_center, w, h]\n",
    "    to [x_min, y_min, x_max, y_max].\n",
    "    \"\"\"\n",
    "    if len(bbox) == 5:\n",
    "        # old: [class_id, x_center, y_center, width, height]\n",
    "        class_id, x_center, y_center, width, height = bbox\n",
    "    else:\n",
    "        # new: [prefix, class_id, x_center, y_center, width, height]\n",
    "        _, class_id, x_center, y_center, width, height = bbox\n",
    "    \n",
    "    x_min = (x_center - width / 2) * img_width\n",
    "    y_min = (y_center - height / 2) * img_height\n",
    "    x_max = (x_center + width / 2) * img_width\n",
    "    y_max = (y_center + height / 2) * img_height\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x_min = max(box1[0], box2[0])\n",
    "    y_min = max(box1[1], box2[1])\n",
    "    x_max = min(box1[2], box2[2])\n",
    "    y_max = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area_box1 + area_box2 - intersection\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def categorize_images(labels, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Categorize images based on bounding box area ratio:\n",
    "      <=1%, 1%-5%, >5%\n",
    "    Each bounding box is either [prefix, class_id, x_c, y_c, w, h]\n",
    "    or [class_id, x_c, y_c, w, h].\n",
    "    \"\"\"\n",
    "    categories = {\"<=1%\": [], \"1%-5%\": [], \">5%\": []}\n",
    "    \n",
    "    for image_name, bboxes in labels.items():\n",
    "        for bbox in bboxes:\n",
    "            if len(bbox) == 5:\n",
    "                class_id, x_c, y_c, w, h = bbox\n",
    "            else:\n",
    "                prefix, class_id, x_c, y_c, w, h = bbox\n",
    "            \n",
    "            width_pixels  = w * img_width\n",
    "            height_pixels = h * img_height\n",
    "            area_ratio = (width_pixels * height_pixels) / (img_width * img_height)\n",
    "\n",
    "            if area_ratio <= 0.01:\n",
    "                categories[\"<=1%\"].append(image_name)\n",
    "            elif 0.01 < area_ratio <= 0.05:\n",
    "                categories[\"1%-5%\"].append(image_name)\n",
    "            else:\n",
    "                categories[\">5%\"].append(image_name)\n",
    "\n",
    "    return categories\n",
    "\n",
    "def evaluate_detections(ground_truths, detections, img_width, img_height, iou_threshold=0.5, category_images=None):\n",
    "    \"\"\"\n",
    "    Evaluate detections against ground truths using precision, recall, F1 score, and mean IoU.\n",
    "    Duplicated detections that differ only by prefix (e.g., 'y' vs 'y2') are treated as identical.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    ious = []\n",
    "\n",
    "    # If we only want to evaluate a subset of images (based on category)\n",
    "    if category_images is not None:\n",
    "        filtered_ground_truths = {k: v for k, v in ground_truths.items() if k in category_images}\n",
    "        filtered_detections   = {k: v for k, v in detections.items()   if k in category_images}\n",
    "    else:\n",
    "        filtered_ground_truths = ground_truths\n",
    "        filtered_detections   = detections\n",
    "\n",
    "    for image_name, gt_bboxes in filtered_ground_truths.items():\n",
    "        pred_bboxes = filtered_detections.get(image_name, [])\n",
    "        \n",
    "        # Deduplicate predicted bboxes ignoring prefix differences\n",
    "        unique_pred_bboxes = []\n",
    "        seen = set()\n",
    "        for pred_bbox in pred_bboxes:\n",
    "            if len(pred_bbox) == 5:\n",
    "                key = tuple(pred_bbox)\n",
    "            else:\n",
    "                # key ignores the prefix (element 0)\n",
    "                key = (pred_bbox[1], pred_bbox[2], pred_bbox[3], pred_bbox[4], pred_bbox[5])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_pred_bboxes.append(pred_bbox)\n",
    "\n",
    "        matched_gt = set()\n",
    "\n",
    "        for pred_bbox in unique_pred_bboxes:\n",
    "            pred_box_xyxy = yolo_to_xyxy(pred_bbox, img_width, img_height)\n",
    "            match_found = False\n",
    "\n",
    "            for i, gt_bbox in enumerate(gt_bboxes):\n",
    "                if i in matched_gt:\n",
    "                    continue\n",
    "\n",
    "                gt_box_xyxy = yolo_to_xyxy(gt_bbox, img_width, img_height)\n",
    "                iou = calculate_iou(pred_box_xyxy, gt_box_xyxy)\n",
    "                if iou >= iou_threshold:\n",
    "                    tp += 1\n",
    "                    matched_gt.add(i)\n",
    "                    match_found = True\n",
    "                    ious.append(iou)\n",
    "                    break\n",
    "\n",
    "            if not match_found:\n",
    "                fp += 1\n",
    "\n",
    "        fn += (len(gt_bboxes) - len(matched_gt))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1        = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    mean_iou  = np.mean(ious) if ious else 0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Mean IoU\": mean_iou,\n",
    "        \"True Positives\": tp,\n",
    "        \"False Positives\": fp,\n",
    "        \"False Negatives\": fn,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #  your folder paths and loading code \n",
    "    ground_truth_folder = r\"C:\\Users\\michael\\Desktop\\f_proj\\output_xml\\test\"\n",
    "    yolo_only_folder    = r\"C:\\Users\\michael\\Desktop\\f_proj\\YOLO_test_resultsoriginal\\predict\\labels\"\n",
    "    hs_pipeline_folder  = r\"C:\\Users\\michael\\Desktop\\f_proj\\testresults\\labels\"\n",
    "    img_width, img_height = 640, 480\n",
    "\n",
    "    # 1) Load\n",
    "    ground_truths = load_xml_ground_truths(ground_truth_folder, img_width, img_height, class_mapping={\"drone\": 0})\n",
    "    yolo_only_detections   = load_yolo_labels(yolo_only_folder,    img_width=img_width, img_height=img_height)\n",
    "    hs_pipeline_detections = load_yolo_labels(hs_pipeline_folder, normalize=True, img_width=img_width, img_height=img_height)\n",
    "\n",
    "    # 2) Categorize by size\n",
    "    categories = categorize_images(ground_truths, img_width, img_height)\n",
    "\n",
    "    # 3) Evaluate with CIs\n",
    "    for category, image_list in categories.items():\n",
    "        print(f\"\\nMetrics for category: {category} (Image Count = {len(image_list)})\")\n",
    "\n",
    "        # YOLO-only\n",
    "        yolo_metrics = evaluate_detections(\n",
    "            ground_truths,\n",
    "            yolo_only_detections,\n",
    "            img_width,\n",
    "            img_height,\n",
    "            category_images=image_list\n",
    "        )\n",
    "        tp = yolo_metrics[\"True Positives\"]\n",
    "        fp = yolo_metrics[\"False Positives\"]\n",
    "        fn = yolo_metrics[\"False Negatives\"]\n",
    "\n",
    "        # 95% Wilson CIs\n",
    "        prec_n = tp + fp\n",
    "        rec_n  = tp + fn\n",
    "        prec_low, prec_high = proportion_confint(tp, prec_n, method=\"wilson\") if prec_n>0 else (0,0)\n",
    "        rec_low,  rec_high  = proportion_confint(tp, rec_n, method=\"wilson\") if rec_n>0 else (0,0)\n",
    "\n",
    "        print(\"YOLO-Only:\")\n",
    "        print(f\"  Precision = {yolo_metrics['Precision']:.2f} [{prec_low:.2f}, {prec_high:.2f}]\")\n",
    "        print(f\"  Recall    = {yolo_metrics['Recall']:.2f} [{rec_low:.2f}, {rec_high:.2f}]\")\n",
    "        print(f\"  F1-Score  = {yolo_metrics['F1-Score']:.2f}\")\n",
    "        print(f\"  Mean IoU  = {yolo_metrics['Mean IoU']:.2f}\")\n",
    "        print(f\"  TP = {tp}, FP = {fp}, FN = {fn}\")\n",
    "\n",
    "        # YOLO + HS pipeline\n",
    "        pipe_metrics = evaluate_detections(\n",
    "            ground_truths,\n",
    "            hs_pipeline_detections,\n",
    "            img_width,\n",
    "            img_height,\n",
    "            category_images=image_list\n",
    "        )\n",
    "        tp = pipe_metrics[\"True Positives\"]\n",
    "        fp = pipe_metrics[\"False Positives\"]\n",
    "        fn = pipe_metrics[\"False Negatives\"]\n",
    "\n",
    "        prec_n = tp + fp\n",
    "        rec_n  = tp + fn\n",
    "        prec_low, prec_high = proportion_confint(tp, prec_n, method=\"wilson\") if prec_n>0 else (0,0)\n",
    "        rec_low,  rec_high  = proportion_confint(tp, rec_n, method=\"wilson\") if rec_n>0 else (0,0)\n",
    "\n",
    "        print(\"YOLO + HS Pipeline:\")\n",
    "        print(f\"  Precision = {pipe_metrics['Precision']:.2f} [{prec_low:.2f}, {prec_high:.2f}]\")\n",
    "        print(f\"  Recall    = {pipe_metrics['Recall']:.2f} [{rec_low:.2f}, {rec_high:.2f}]\")\n",
    "        print(f\"  F1-Score  = {pipe_metrics['F1-Score']:.2f}\")\n",
    "        print(f\"  Mean IoU  = {pipe_metrics['Mean IoU']:.2f}\")\n",
    "        print(f\"  TP = {tp}, FP = {fp}, FN = {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: 0.00115966875 0.0041688375 9.765625e-05 0.0002907979166666667\n",
      "Normalized: 0.00194091875 0.0028363708333333333 0.000263671875 0.0004904520833333333\n",
      "Normalized: 0.0018359375000000001 0.0022482645833333333 0.0001171875 0.0003385416666666667\n",
      "Overall YOLO-Only Metrics:\n",
      "  Precision = 0.85 [0.81, 0.88]\n",
      "  Recall    = 0.69 [0.65, 0.73]\n",
      "  F1-Score  = 0.76\n",
      "  Mean IoU  = 0.77\n",
      "  TP = 335, FP = 58, FN = 152\n",
      "\n",
      "Overall YOLO + HS Pipeline Metrics:\n",
      "  Precision = 0.80 [0.76, 0.83]\n",
      "  Recall    = 0.73 [0.69, 0.77]\n",
      "  F1-Score  = 0.76\n",
      "  Mean IoU  = 0.77\n",
      "  TP = 355, FP = 90, FN = 132\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set your folder paths and image dimensions\n",
    "    ground_truth_folder = r\"C:\\Users\\michael\\Desktop\\f_proj\\output_xml\\test\"\n",
    "    yolo_only_folder    = r\"C:\\Users\\michael\\Desktop\\f_proj\\YOLO_test_resultsoriginal\\predict\\labels\"\n",
    "    hs_pipeline_folder  = r\"C:\\Users\\michael\\Desktop\\f_proj\\testresults\\labels\"\n",
    "    img_width, img_height = 640, 480\n",
    "\n",
    "    # Load ground truth annotations from XML (e.g., Pascal VOC format)\n",
    "    ground_truths = load_xml_ground_truths(\n",
    "        ground_truth_folder,\n",
    "        img_width,\n",
    "        img_height,\n",
    "        class_mapping={\"drone\": 0}\n",
    "    )\n",
    "    \n",
    "    # Load YOLO-format detections\n",
    "    yolo_only_detections = load_yolo_labels(\n",
    "        yolo_only_folder, \n",
    "        img_width=img_width, \n",
    "        img_height=img_height\n",
    "    )\n",
    "    hs_pipeline_detections = load_yolo_labels(\n",
    "        hs_pipeline_folder, \n",
    "        normalize=True, \n",
    "        img_width=img_width, \n",
    "        img_height=img_height\n",
    "    )\n",
    "    \n",
    "    # Compute overall evaluation metrics (no category filtering)\n",
    "    overall_yolo = evaluate_detections(\n",
    "        ground_truths,\n",
    "        yolo_only_detections,\n",
    "        img_width,\n",
    "        img_height\n",
    "    )\n",
    "    overall_pipeline = evaluate_detections(\n",
    "        ground_truths,\n",
    "        hs_pipeline_detections,\n",
    "        img_width,\n",
    "        img_height\n",
    "    )\n",
    "    \n",
    "    # Helper to print a metric with its 95% Wilson CI\n",
    "    def print_with_ci(name, tp, fp, fn):\n",
    "        # precision CI\n",
    "        prec_n = tp + fp\n",
    "        prec_low, prec_high = (0,0)\n",
    "        if prec_n > 0:\n",
    "            prec_low, prec_high = proportion_confint(tp, prec_n, method=\"wilson\")\n",
    "        # recall CI\n",
    "        rec_n = tp + fn\n",
    "        rec_low, rec_high = (0,0)\n",
    "        if rec_n > 0:\n",
    "            rec_low, rec_high = proportion_confint(tp, rec_n, method=\"wilson\")\n",
    "        # point estimates\n",
    "        precision = tp / prec_n if prec_n>0 else 0\n",
    "        recall    = tp / rec_n if rec_n>0 else 0\n",
    "\n",
    "        print(f\"  Precision = {precision:.2f} [{prec_low:.2f}, {prec_high:.2f}]\")\n",
    "        print(f\"  Recall    = {recall:.2f} [{rec_low:.2f}, {rec_high:.2f}]\")\n",
    "    \n",
    "    # Print overall results for YOLO-only\n",
    "    print(\"Overall YOLO-Only Metrics:\")\n",
    "    tp, fp, fn = overall_yolo[\"True Positives\"], overall_yolo[\"False Positives\"], overall_yolo[\"False Negatives\"]\n",
    "    print_with_ci(\"YOLO\", tp, fp, fn)\n",
    "    print(f\"  F1-Score  = {overall_yolo['F1-Score']:.2f}\")\n",
    "    print(f\"  Mean IoU  = {overall_yolo['Mean IoU']:.2f}\")\n",
    "    print(f\"  TP = {tp}, FP = {fp}, FN = {fn}\")\n",
    "    \n",
    "    # Print overall results for YOLO + HS pipeline\n",
    "    print(\"\\nOverall YOLO + HS Pipeline Metrics:\")\n",
    "    tp, fp, fn = overall_pipeline[\"True Positives\"], overall_pipeline[\"False Positives\"], overall_pipeline[\"False Negatives\"]\n",
    "    print_with_ci(\"Pipeline\", tp, fp, fn)\n",
    "    print(f\"  F1-Score  = {overall_pipeline['F1-Score']:.2f}\")\n",
    "    print(f\"  Mean IoU  = {overall_pipeline['Mean IoU']:.2f}\")\n",
    "    print(f\"  TP = {tp}, FP = {fp}, FN = {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resultss for drones only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by drone size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: 0.00115966875 0.0041688375 9.765625e-05 0.0002907979166666667\n",
      "Normalized: 0.00194091875 0.0028363708333333333 0.000263671875 0.0004904520833333333\n",
      "Normalized: 0.0018359375000000001 0.0022482645833333333 0.0001171875 0.0003385416666666667\n",
      "\n",
      "Metrics for category: <=1% (Image Count = 183)\n",
      "YOLO-Only (Drones) Metrics:\n",
      "  Precision: 0.73\n",
      "  Recall: 0.53\n",
      "  F1-Score: 0.61\n",
      "  Mean IoU: 0.69\n",
      "  True Positives: 98\n",
      "  False Positives: 36\n",
      "  False Negatives: 87\n",
      "  mAP: 0.73\n",
      "YOLO + HS Pipeline (Drones) Metrics:\n",
      "  Precision: 0.65\n",
      "  Recall: 0.57\n",
      "  F1-Score: 0.61\n",
      "  Mean IoU: 0.69\n",
      "  True Positives: 106\n",
      "  False Positives: 58\n",
      "  False Negatives: 79\n",
      "  mAP: 0.65\n",
      "\n",
      "Metrics for category: 1%-5% (Image Count = 154)\n",
      "YOLO-Only (Drones) Metrics:\n",
      "  Precision: 0.94\n",
      "  Recall: 0.66\n",
      "  F1-Score: 0.78\n",
      "  Mean IoU: 0.78\n",
      "  True Positives: 105\n",
      "  False Positives: 7\n",
      "  False Negatives: 53\n",
      "  mAP: 0.94\n",
      "YOLO + HS Pipeline (Drones) Metrics:\n",
      "  Precision: 0.91\n",
      "  Recall: 0.73\n",
      "  F1-Score: 0.81\n",
      "  Mean IoU: 0.78\n",
      "  True Positives: 115\n",
      "  False Positives: 12\n",
      "  False Negatives: 43\n",
      "  mAP: 0.91\n",
      "\n",
      "Metrics for category: >5% (Image Count = 150)\n",
      "YOLO-Only (Drones) Metrics:\n",
      "  Precision: 0.97\n",
      "  Recall: 0.82\n",
      "  F1-Score: 0.89\n",
      "  Mean IoU: 0.83\n",
      "  True Positives: 126\n",
      "  False Positives: 4\n",
      "  False Negatives: 27\n",
      "  mAP: 0.97\n",
      "YOLO + HS Pipeline (Drones) Metrics:\n",
      "  Precision: 0.91\n",
      "  Recall: 0.84\n",
      "  F1-Score: 0.87\n",
      "  Mean IoU: 0.82\n",
      "  True Positives: 129\n",
      "  False Positives: 13\n",
      "  False Negatives: 24\n",
      "  mAP: 0.91\n"
     ]
    }
   ],
   "source": [
    "def categorize_images(labels, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Categorize images based on bounding box area ratio:\n",
    "      <=1%, 1%-5%, >5%\n",
    "      \n",
    "    Supports bounding boxes in either format:\n",
    "      - [prefix, class_id, x_center, y_center, width, height] (e.g., from XML ground truths)\n",
    "      - [prefix, class_id, conf, x_center, y_center, width, height] (from detections)\n",
    "    \"\"\"\n",
    "    categories = {\"<=1%\": [], \"1%-5%\": [], \">5%\": []}\n",
    "    \n",
    "    for image_name, bboxes in labels.items():\n",
    "        for bbox in bboxes:\n",
    "            if len(bbox) == 6:\n",
    "                # Format from XML ground truths: [prefix, class_id, x_center, y_center, w, h]\n",
    "                _, _, x_c, y_c, w, h = bbox\n",
    "            elif len(bbox) == 7:\n",
    "                # Format from detections: [prefix, class_id, conf, x_c, y_c, w, h]\n",
    "                _, _, _, x_c, y_c, w, h = bbox\n",
    "            else:\n",
    "                print(f\"Unexpected bbox format for image {image_name}: {bbox}\")\n",
    "                continue\n",
    "\n",
    "            width_pixels  = w * img_width\n",
    "            height_pixels = h * img_height\n",
    "            area_ratio = (width_pixels * height_pixels) / (img_width * img_height)\n",
    "\n",
    "            if area_ratio <= 0.01:\n",
    "                categories[\"<=1%\"].append(image_name)\n",
    "            elif 0.01 < area_ratio <= 0.05:\n",
    "                categories[\"1%-5%\"].append(image_name)\n",
    "            else:\n",
    "                categories[\">5%\"].append(image_name)\n",
    "                \n",
    "    return categories\n",
    "# Folder paths for XML ground truth and YOLO-format prediction files.\n",
    "ground_truth_folder = r\"C:\\Users\\michael\\Desktop\\f_proj\\output_xml\\test\"\n",
    "yolo_only_folder    = r\"C:\\Users\\michael\\Desktop\\f_proj\\YOLO_test_resultsoriginal\\predict\\labels\"\n",
    "hs_pipeline_folder  = r\"C:\\Users\\michael\\Desktop\\f_proj\\testresults\\labels\"\n",
    "\n",
    "# Set image dimensions (must match annotations and detections)\n",
    "img_width, img_height = 640, 480\n",
    "\n",
    "# 1. Load ground truths from XML files\n",
    "ground_truths_all = load_xml_ground_truths(\n",
    "    ground_truth_folder,\n",
    "    img_width,\n",
    "    img_height,\n",
    "    class_mapping={\"drone\": 0}  # Adjust if needed\n",
    ")\n",
    "\n",
    "# 2. Load YOLO-format detections (from text files)\n",
    "yolo_only_all   = load_yolo_labels(yolo_only_folder, img_width=img_width, img_height=img_height)\n",
    "hs_pipeline_all = load_yolo_labels(hs_pipeline_folder, normalize=True, img_width=img_width, img_height=img_height)\n",
    "\n",
    "# 3. Filter to keep only drone detections (drone: class_id==0)\n",
    "# For ground truths, the prefix is None; ensure it's allowed.\n",
    "ground_truths_drones = filter_to_drones_only(ground_truths_all, prefixes=(None,))\n",
    "yolo_only_drones     = filter_to_drones_only(yolo_only_all, prefixes=('y', 'y2', None))\n",
    "hs_pipeline_drones   = filter_to_drones_only(hs_pipeline_all, prefixes=('y', 'y2', 'p', None))\n",
    "\n",
    "# 4. Categorize images by the normalized area of the drone bounding boxes.\n",
    "categories = categorize_images(ground_truths_drones, img_width, img_height)\n",
    "\n",
    "# 5. Evaluate detections (and compute mAP) for each size category.\n",
    "for category, image_list in categories.items():\n",
    "    print(f\"\\nMetrics for category: {category} (Image Count = {len(image_list)})\")\n",
    "\n",
    "    # YOLO-only evaluation for images in this category.\n",
    "    yolo_only_metrics = evaluate_detections(\n",
    "        ground_truths_drones,\n",
    "        yolo_only_drones,\n",
    "        img_width,\n",
    "        img_height,\n",
    "        category_images=image_list\n",
    "    )\n",
    "    mAP_yolo = compute_average_precision(\n",
    "        {k: v for k, v in ground_truths_drones.items() if k in image_list},\n",
    "        {k: v for k, v in yolo_only_drones.items() if k in image_list},\n",
    "        img_width,\n",
    "        img_height\n",
    "    )\n",
    "    print(\"YOLO-Only (Drones) Metrics:\")\n",
    "    for metric, value in yolo_only_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    print(f\"  mAP: {mAP_yolo:.2f}\")\n",
    "\n",
    "    # YOLO + HS Pipeline evaluation for images in this category.\n",
    "    hs_pipeline_metrics = evaluate_detections(\n",
    "        ground_truths_drones,\n",
    "        hs_pipeline_drones,\n",
    "        img_width,\n",
    "        img_height,\n",
    "        category_images=image_list\n",
    "    )\n",
    "    mAP_pipeline = compute_average_precision(\n",
    "        {k: v for k, v in ground_truths_drones.items() if k in image_list},\n",
    "        {k: v for k, v in hs_pipeline_drones.items() if k in image_list},\n",
    "        img_width,\n",
    "        img_height\n",
    "    )\n",
    "    print(\"YOLO + HS Pipeline (Drones) Metrics:\")\n",
    "    for metric, value in hs_pipeline_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    print(f\"  mAP: {mAP_pipeline:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: 0.00115966875 0.0041688375 9.765625e-05 0.0002907979166666667\n",
      "Normalized: 0.00194091875 0.0028363708333333333 0.000263671875 0.0004904520833333333\n",
      "Normalized: 0.0018359375000000001 0.0022482645833333333 0.0001171875 0.0003385416666666667\n",
      "Overall YOLO-Only (Drones) Metrics:\n",
      "  Precision: 0.88\n",
      "  Recall: 0.67\n",
      "  F1-Score: 0.76\n",
      "  Mean IoU: 0.77\n",
      "  True Positives: 325\n",
      "  False Positives: 46\n",
      "  False Negatives: 162\n",
      "  mAP: 0.88\n",
      "\n",
      "Overall YOLO + HS Pipeline (Drones) Metrics:\n",
      "  Precision: 0.81\n",
      "  Recall: 0.71\n",
      "  F1-Score: 0.75\n",
      "  Mean IoU: 0.77\n",
      "  True Positives: 345\n",
      "  False Positives: 82\n",
      "  False Negatives: 142\n",
      "  mAP: 0.81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Folder paths for XML ground truth and YOLO-format predictions.\n",
    "ground_truth_folder = r\"C:\\Users\\michael\\Desktop\\f_proj\\output_xml\\test\"\n",
    "    # YOLO detection results remain in text format:\n",
    "yolo_only_folder    = r\"C:\\Users\\michael\\Desktop\\f_proj\\YOLO_test_resultsoriginal\\predict\\labels\"\n",
    "hs_pipeline_folder  = r\"C:\\Users\\michael\\Desktop\\f_proj\\testresults\\labels\"\n",
    "\n",
    "# Set image dimensions.\n",
    "img_width, img_height = 640, 480\n",
    "\n",
    "# 1. Load ground truths from XML\n",
    "ground_truths_all = load_xml_ground_truths(\n",
    "    ground_truth_folder,\n",
    "    img_width,\n",
    "    img_height,\n",
    "    class_mapping={\"drone\": 0}\n",
    ")\n",
    "\n",
    "# 2. Load YOLO-format detections\n",
    "yolo_only_all   = load_yolo_labels(yolo_only_folder, img_width=img_width, img_height=img_height)\n",
    "hs_pipeline_all = load_yolo_labels(hs_pipeline_folder, normalize=True, img_width=img_width, img_height=img_height)\n",
    "\n",
    "# 3. Filter to keep only drone detections.\n",
    "# Ground truths coming from XML have prefix None.\n",
    "ground_truths_drones = filter_to_drones_only(ground_truths_all, prefixes=(None,))\n",
    "yolo_only_drones     = filter_to_drones_only(yolo_only_all, prefixes=('y', 'y2', None))\n",
    "hs_pipeline_drones   = filter_to_drones_only(hs_pipeline_all, prefixes=('y', 'y2', 'p', None))\n",
    "\n",
    "# 4. Overall evaluation (all images)\n",
    "overall_yolo_metrics = evaluate_detections(\n",
    "    ground_truths_drones,\n",
    "    yolo_only_drones,\n",
    "    img_width,\n",
    "    img_height\n",
    ")\n",
    "overall_yolo_mAP = compute_average_precision(\n",
    "    ground_truths_drones,\n",
    "    yolo_only_drones,\n",
    "    img_width,\n",
    "    img_height\n",
    ")\n",
    "\n",
    "print(\"Overall YOLO-Only (Drones) Metrics:\")\n",
    "for metric, value in overall_yolo_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")\n",
    "print(f\"  mAP: {overall_yolo_mAP:.2f}\")\n",
    "\n",
    "overall_pipeline_metrics = evaluate_detections(\n",
    "    ground_truths_drones,\n",
    "    hs_pipeline_drones,\n",
    "    img_width,\n",
    "    img_height\n",
    ")\n",
    "overall_pipeline_mAP = compute_average_precision(\n",
    "    ground_truths_drones,\n",
    "    hs_pipeline_drones,\n",
    "    img_width,\n",
    "    img_height\n",
    ")\n",
    "\n",
    "print(\"\\nOverall YOLO + HS Pipeline (Drones) Metrics:\")\n",
    "for metric, value in overall_pipeline_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")\n",
    "print(f\"  mAP: {overall_pipeline_mAP:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
